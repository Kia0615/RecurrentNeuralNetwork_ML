{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72193, 1)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/kia/Desktop/SYS 6016/HW/hw4/'\n",
    "file = 'train.txt'\n",
    "corpus = pd.read_csv(path+file, sep=r'\\s{2,}',header=None)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = corpus.head(100)\n",
    "train_corpus=np.expand_dims(train_corpus.values,1)\n",
    "train_corpus = train_corpus.T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert corpus to list\n",
    "reviews = list(train_corpus)\n",
    "reviews = [str(review) for review in reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20519"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split into sentences\n",
    "lines = [re.split('[?.!]', review.lower()) for review in reviews]\n",
    "#strip whitespace\n",
    "lines = [[sentence.strip(' ') for sentence in sentences] for sentences in lines]\n",
    "#combine sentences from different documents\n",
    "sentences = [j for i in lines for j in i]\n",
    "#split sentences into words\n",
    "words = [re.split(' ', sentence) for sentence in sentences]\n",
    "words = str(words).split()\n",
    "words = [''.join(e for e in string if e.isalnum()) for string in words]\n",
    "words = [''.join(e for e in string if not e.isdigit()) for string in words]\n",
    "# remove empty strings\n",
    "words = list(filter(None, words)) \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 20518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['we', 'brought']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(1, len(words)):\n",
    "    sequence = words[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4408"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(words))\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for i, unique_word in enumerate(unique_words):\n",
    "  vocab[unique_word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20518"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_encodings = []\n",
    "for sequence in sequences:\n",
    "    sequence_encoding = [vocab[sequence[0]],vocab[sequence[1]]]\n",
    "    sequence_encodings.append(sequence_encoding)\n",
    "len(sequence_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data to feed to model:\n",
    "# each observation will be of length 'seq_size'\n",
    "# For example, seq_size=2 then\n",
    "# x[0]=[t_0,t_1]\n",
    "# x[1]=[t_1,t_2]\n",
    "# ...\n",
    "# As for Y, it will also be of length 'seq_size'. why? (hint, sequence-to-sequence)\n",
    "# For example, seq_size=2 then\n",
    "# y[0]=[t_1,t_2]\n",
    "# y[1]=[t_2,t_3]\n",
    "# ...\n",
    "seq_size = 1 \n",
    "trainX, trainY = [], []\n",
    "for i in range(len(sequence_encodings) - seq_size):\n",
    "    trainX.append(np.expand_dims(sequence_encodings[i:i+seq_size], axis=1).tolist())\n",
    "    trainY.append(sequence_encodings[i+1:i+seq_size+1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[208], [3900]]\n",
      "[[3900], [1796]]\n",
      "Y:\n",
      " [3900, 1796]\n",
      "[1796, 1166]\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = [], []\n",
    "seq_size=2\n",
    "for i in range(1,len(sequence_encodings)):\n",
    "    trainX.append(np.expand_dims(sequence_encodings[i-1], axis=1).tolist())\n",
    "    trainY.append(sequence_encodings[i])\n",
    "# Inspect trainX and trainY\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:2]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [208, 3900]\n",
      "[3900, 1796]\n",
      "Y:\n",
      " [3900, 1796]\n",
      "[1796, 1166]\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = [], []\n",
    "seq_size=2\n",
    "for i in range(1,len(sequence_encodings)):\n",
    "    trainX.append(sequence_encodings[i-1])\n",
    "    trainY.append(sequence_encodings[i])\n",
    "# Inspect trainX and trainY\n",
    "print(\"X:\\n\",'\\n'.join([str(item) for item in trainX[0:2]]))\n",
    "print(\"Y:\\n\",'\\n'.join([str(item) for item in trainY[0:2]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build computational graph\n",
    "input_dim=1 # dim > 1 for multivariate time series\n",
    "hidden_dim=100 # number of hiddent units h\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input place holders\n",
    "    # input Shape: [# training examples, sequence length, # features]\n",
    "    x = tf.placeholder(tf.float32,[None,seq_size,input_dim])\n",
    "    # label Shape: [# training examples, sequence length]\n",
    "    y = tf.placeholder(tf.float32,[None,seq_size])\n",
    "    \n",
    "    # RNN Network\n",
    "    cell = rnn.BasicRNNCell(hidden_dim)\n",
    "    \n",
    "    # RNN output Shape: [# training examples, sequence length, # hidden] \n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # weights for output dense layer (i.e., after RNN)\n",
    "    # W shape: [# hidden, 1]\n",
    "    W_out = tf.Variable(tf.random_normal([hidden_dim,1]),name=\"w_out\") \n",
    "    # b shape: [1]\n",
    "    b_out = tf.Variable(tf.random_normal([1]),name=\"b_out\")\n",
    "\n",
    "    # output dense layer:\n",
    "    num_examples = tf.shape(x)[0] \n",
    "    # convert W from [# hidden, 1] to [# training examples, # hidden, 1]\n",
    "    # step 1: add a new dimension at index 0 using tf.expand_dims\n",
    "    w_exp= tf.expand_dims(W_out,0)\n",
    "    # step 2: duplicate W for 'num_examples' times using tf.tile\n",
    "    W_repeated = tf.tile(w_exp,[num_examples,1,1])\n",
    "    \n",
    "    # Dense Layer calculation: \n",
    "    # [# training examples, sequence length, # hidden] *\n",
    "    # [# training examples, # hidden, 1] = [# training examples, sequence length]\n",
    "    \n",
    "    y_pred = tf.matmul(outputs,W_repeated)+b_out\n",
    "    # Actually, y_pred: [# training examples, sequence length, 1]\n",
    "    # Remove last dimension using tf.squeeze\n",
    "    y_pred = tf.squeeze(y_pred)\n",
    "    \n",
    "    # Cost & Training Step\n",
    "    cost = tf.reduce_mean(tf.square(y_pred-y))\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "  step, train err=      0: 6650120.50000\n",
      "  step, train err=    100: 5920318.50000\n",
      "  step, train err=    200: 5519335.50000\n",
      "  step, train err=    300: 5146309.50000\n",
      "  step, train err=    400: 4799859.00000\n",
      "  step, train err=    500: 4478690.00000\n",
      "  step, train err=    600: 4181554.75000\n",
      "  step, train err=    700: 3907129.50000\n",
      "  step, train err=    800: 3654325.50000\n",
      "  step, train err=    900: 3421941.00000\n",
      "  step, train err=   1000: 3208951.25000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-1ec04cbca918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Compute MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# step 1: denormalize data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpredicted_vals_dnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_vals\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_dataset\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmin_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m# step 2: get ground-truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mactual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Run Session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Run for 1000 iterations (1000 is arbitrary, need a validation set to tune!)\n",
    "    print('Training...')\n",
    "    for i in range(1000): # If we train more, would we overfit? Try 10000\n",
    "        _, train_err = sess.run([train_op,cost],feed_dict={x:trainX,y:trainY})\n",
    "        if i==0:\n",
    "            print('  step, train err= %6d: %8.5f' % (0,train_err)) \n",
    "        elif  (i+1) % 100 == 0: \n",
    "            print('  step, train err= %6d: %8.5f' % (i+1,train_err)) \n",
    "\n",
    "    # Test trained model on training data\n",
    "    predicted_vals_all= sess.run(y_pred,feed_dict={x:trainX}) \n",
    "    # Get last item in each predicted sequence:\n",
    "    predicted_vals = predicted_vals_all[:,seq_size-1]\n",
    "    \n",
    "    # Compute MSE\n",
    "    # step 1: denormalize data\n",
    "    predicted_vals_dnorm=predicted_vals*(max_dataset-min_dataset)+min_dataset\n",
    "    # step 2: get ground-truth\n",
    "    actual=dataset[seq_size:][:,0]\n",
    "    # step 3: compute MSE\n",
    "    mse = ((predicted_vals_dnorm - actual) ** 2).mean()\n",
    "    print(\"Training MSE = %10.5f\"%mse)\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(seq_size,seq_size+len(predicted_vals_dnorm))), predicted_vals_dnorm, color='r', label='predicted')\n",
    "    plt.plot(list(range(len(dataset))), dataset, color='g', label='actual')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow1.7)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
